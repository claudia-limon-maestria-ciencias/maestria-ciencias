{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMk/vx8iv+mhkqZ/d6N466",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-limon-maestria-ciencias/maestria-ciencias/blob/main/langchain_model_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cFHh9B1UKFZJ"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-community\n",
        "!pip install langchain-anthropic\n",
        "!pip install langchain-google-genai\n",
        "!pip install rouge-score\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install unstructured\n",
        "!pip install pandas numpy tabulate sentence-transformers nltk scikit-learn openpyxl\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "foiBtp9yKbHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "hWeonIf8Kz6l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    {\"path\": \"/content/drive/MyDrive/articulo_langchain/codigompalags_libro_sexto.pdf\", \"format\": \"pdf\"},\n",
        "    {\"path\": \"/content/drive/MyDrive/articulo_langchain/codigompalags_librosexto_txt4.txt\", \"format\": \"txt\"}\n",
        "]\n",
        "\n",
        "# File where we will save the texts of the chunks (Matches variable in refactored code)\n",
        "CHUNKS_TEXTS_FILE = 'chunks_texts.json'\n",
        "\n",
        "# File where we will save the names of the created collections (Matches variable in refactored code)\n",
        "COLLECTIONS_FILE = 'created_collections.json'\n",
        "\n",
        "# Embedding types (Matches variable in refactored code)\n",
        "embedding_types = [\"openai\", \"google\"]"
      ],
      "metadata": {
        "id": "RBtl6mPzM9gF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "retrievers = {}\n",
        "created_collections = {} # Start with empty dict, will be populated\n",
        "all_chunks_data = {}     # Start with empty dict, will be populated\n",
        "\n",
        "if os.path.exists(COLLECTIONS_FILE):\n",
        "    try:\n",
        "        with open(COLLECTIONS_FILE, 'r', encoding='utf-8') as f:\n",
        "            created_collections = json.load(f)\n",
        "        print(f\"Loaded {len(created_collections)} existing collection names from '{COLLECTIONS_FILE}' (will be updated).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not read existing collections file '{COLLECTIONS_FILE}': {e}. Starting fresh.\")\n",
        "        created_collections = {}\n",
        "\n",
        "if os.path.exists(CHUNKS_TEXTS_FILE):\n",
        "    try:\n",
        "        with open(CHUNKS_TEXTS_FILE, 'r', encoding='utf-8') as f:\n",
        "             all_chunks_data = json.load(f)\n",
        "        print(f\"Loaded existing chunk data for {len(all_chunks_data)} collections from '{CHUNKS_TEXTS_FILE}' (will be updated).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not read existing chunks file '{CHUNKS_TEXTS_FILE}': {e}. Starting fresh.\")\n",
        "        all_chunks_data = {}\n",
        "# --- End of setup ---\n",
        "\n",
        "\n",
        "# --- Main Processing Loop ---\n",
        "for document in documents:\n",
        "    # Validate path existence\n",
        "    doc_path = document.get(\"path\", \"\")\n",
        "    if not os.path.exists(doc_path):\n",
        "        print(f\"Error: The path does not exist or was not specified for '{doc_path}'. Skipping.\")\n",
        "        continue\n",
        "    doc_name = os.path.basename(doc_path)\n",
        "\n",
        "    for embedding_type in embedding_types:\n",
        "        # Unique key for this retriever (still useful for the retrievers dictionary)\n",
        "        key = f\"{doc_name}_{embedding_type}\"\n",
        "        print(f\"\\n--- Processing: {key} ---\")\n",
        "\n",
        "        # --- Initialize embeddings ---\n",
        "        print(f\"Initializing {embedding_type} embeddings...\")\n",
        "        embeddings = None # Initialize\n",
        "        try:\n",
        "            if embedding_type == \"openai\":\n",
        "                embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
        "            elif embedding_type == \"google\":\n",
        "                embeddings = GoogleGenerativeAIEmbeddings(\n",
        "                    model=\"models/embedding-001\",\n",
        "                    api_key=GOOGLE_API_KEY\n",
        "                )\n",
        "            else:\n",
        "                 print(f\"⚠️ Warning: Unknown embedding type '{embedding_type}'. Skipping.\")\n",
        "                 continue # Skip to next embedding type if unknown\n",
        "            if embeddings is None: # Should not happen if type is known, but safety check\n",
        "                 print(f\"Error: Failed to initialize embeddings for {embedding_type}. Skipping.\")\n",
        "                 continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing embeddings for {embedding_type}: {e}. Skipping.\")\n",
        "            continue\n",
        "        # --- End embedding initialization ---\n",
        "\n",
        "        vector_store = None # Initialize vector_store variable for this iteration\n",
        "\n",
        "        # Create New Vector Database\n",
        "        try:\n",
        "            print(f\"Attempting to create new vector database for {key}...\")\n",
        "\n",
        "            # Create a unique name for this ChromaDB collection (with timestamp)\n",
        "            safe_doc_name = \"\".join(c if c.isalnum() or c in ['-', '_'] else '_' for c in doc_name)\n",
        "            # Ensure collection name is valid for Chroma\n",
        "            collection_name = f\"{safe_doc_name}_{embedding_type}_{int(time.time())}\"\n",
        "            # Basic Chroma name validation (adjust if needed based on Chroma docs)\n",
        "            collection_name = collection_name.replace('..', '_') # Avoid consecutive dots\n",
        "            if not (3 <= len(collection_name) <= 63):\n",
        "                 collection_name = f\"col_{int(time.time())}\" # Fallback name if length is wrong\n",
        "                 print(f\"Generated collection name was invalid, using fallback: {collection_name}\")\n",
        "            print(f\"ChromaDB collection name will be: {collection_name}\")\n",
        "\n",
        "\n",
        "            # Load document according to format\n",
        "            print(f\"Loading document: {doc_path}\")\n",
        "            loader = None\n",
        "            if doc_path.lower().endswith('.pdf'):\n",
        "                loader = PyPDFLoader(doc_path)\n",
        "            elif doc_path.lower().endswith('.txt'):\n",
        "                loader = TextLoader(doc_path, encoding='utf-8')\n",
        "            else:\n",
        "                print(f\"Unsupported format for {doc_path}. Skipping creation.\")\n",
        "                continue # Skip this embedding type\n",
        "\n",
        "            loaded_documents = loader.load()\n",
        "            if not loaded_documents:\n",
        "                print(f\"Warning: No documents were loaded from {doc_path}. Skipping creation.\")\n",
        "                continue\n",
        "\n",
        "            # Split into chunks\n",
        "            print(\"   Splitting into chunks...\")\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1500,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "            chunks = splitter.split_documents(loaded_documents)\n",
        "            if not chunks:\n",
        "                print(f\"Warning: No chunks were generated for {doc_path}. Skipping creation.\")\n",
        "                continue\n",
        "            print(f\"   Document split into {len(chunks)} chunks.\")\n",
        "\n",
        "            # --- Save Chunk Texts ---\n",
        "            print(f\"Saving chunk texts in '{CHUNKS_TEXTS_FILE}'...\")\n",
        "            chunk_texts = [chunk.page_content for chunk in chunks]\n",
        "            all_chunks_data[collection_name] = chunk_texts # Use collection_name as key\n",
        "            try:\n",
        "                with open(CHUNKS_TEXTS_FILE, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(all_chunks_data, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"Chunk texts saved successfully for {collection_name}.\")\n",
        "            except Exception as e_json_save:\n",
        "                print(f\"Error saving chunk texts to JSON: {e_json_save}\")\n",
        "            # --- End Save Chunk Texts ---\n",
        "\n",
        "            # --- Create Chroma vector database ---\n",
        "            print(f\"   Creating ChromaDB vector database '{collection_name}'...\")\n",
        "            vector_store = Chroma.from_documents(\n",
        "                 chunks,\n",
        "                 embeddings,\n",
        "                 collection_name=collection_name\n",
        "            )\n",
        "\n",
        "            # --- Save collection name to registry ---\n",
        "            created_collections[key] = collection_name # Map the original key to the new name\n",
        "            try:\n",
        "                with open(COLLECTIONS_FILE, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(created_collections, f, indent=4)\n",
        "                print(f\"Collection name '{collection_name}' registered for key '{key}'.\")\n",
        "            except Exception as e_json_reg:\n",
        "                    print(f\"Error saving collection registry JSON: {e_json_reg}\")\n",
        "            # --- End Save Collection Name ---\n",
        "\n",
        "            print(f\"Vector database '{collection_name}' created successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Fatal error during creation process for {key}: {str(e)}\")\n",
        "            # vector_store remains None if creation failed\n",
        "            continue # Skip to next iteration if creation failed\n",
        "        # --- End Creation Block ---\n",
        "\n",
        "        # --- Create retriever if vector_store was successfully created ---\n",
        "        if vector_store:\n",
        "            try:\n",
        "                 retrievers[key] = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "                 print(f\"Retriever for '{key}' ready.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Error creating retriever for '{key}' from vector store: {e}\")\n",
        "        else:\n",
        "            # This message should now only appear if the creation block had an error\n",
        "            print(f\"No vector store was successfully created for '{key}', cannot create retriever.\")\n",
        "        # --- End retriever creation ---\n",
        "\n",
        "# --- Final Summary ---\n",
        "print(f\"\\n--- Final Summary ---\")\n",
        "print(f\"Collection registry file: {COLLECTIONS_FILE}\")\n",
        "print(f\"Chunk texts file: {CHUNKS_TEXTS_FILE}\")\n",
        "print(f\"{len(retrievers)} retrievers are ready in memory (for the current session).\")\n",
        "print(f\"{len(created_collections)} collections are known (according to collections JSON).\")\n",
        "print(f\"Text data exists for {len(all_chunks_data)} collections (according to chunks JSON).\")\n",
        "# --- End Final Summary ---"
      ],
      "metadata": {
        "id": "6cICs9icK0JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NLP & Similarity ---\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError: # Changed to catch LookupError\n",
        "    print(\"Descargando recursos de NLTK (punkt)...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "print(\"Imported libraries NLTK verified.\")"
      ],
      "metadata": {
        "id": "z1iYsBQjI7wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
        "import time\n",
        "\n",
        "# Evaluation Questions\n",
        "questions = [\n",
        "    \"¿Cuáles son las categorías de superficie para las Tiendas de Productos Básicos según la clasificación?\",\n",
        "    \"¿En qué categorías se clasifican los centros de Educación Superior según su capacidad de ocupantes?\",\n",
        "    \"¿Cómo se clasifican las Torres, Antenas y Chimeneas?\",\n",
        "    \"¿Cuales son las especificaciones para una recámara principal?\",\n",
        "    \"¿Como se conforma el grupo de Deportes y Recreación?\",\n",
        "    \"¿Cuántas camas o consultorios se permiten como máximo para los Hospitales?\",\n",
        "    \"¿Cuál es el ancho mínimo requerido para el acceso principal en edificaciones de tipo Habitación?\",\n",
        "    \"¿Cuáles son los requisitos mínimos para ventilación e iluminación en espacios habitables?\",\n",
        "    \"¿Qué longitud mínima deben tener las áreas de espera techadas en los estacionamientos?\",\n",
        "    \"¿Cuáles son las dimensiones mínimas para un inodoro en un baño publico?\",\n",
        "    \"¿Qué ancho mínimo deben tener las escaleras en edificaciones comerciales?\",\n",
        "    \"¿Cuáles son los requisitos para rampas de accesibilidad?\"\n",
        "]\n",
        "\n",
        "correct_answers = [\n",
        "    \"Hasta 250 m² y más de 250 m²\",\n",
        "    \"Hasta 250 ocupantes y más de 250 ocupantes\",\n",
        "    \"Hasta 8 m de altura, de 8 m hasta 30 m de altura, y más de 30 m de altura\",\n",
        "    \"La recámara principal requiere un área mínima de 7.00 m², con un lado mínimo de 2.50 m y una altura de 2.30 m\",\n",
        "    \"lienzos charros, canchas y centros deportivos, estadios, albercas, plazas de toros, billares, juegos electrónicos o de mesa, hipódromos, autódromos, pistas de patinaje y equitación, y campos de tiro\",\n",
        "    \"Más de 10 camas o consultorios\",\n",
        "    \"0.90 metros\",\n",
        "    \"Los espacios habitables deben tener iluminación y ventilación natural por medio de vanos con área mínima del 17.5% y 5% de la superficie del espacio, respectivamente\",\n",
        "    \"6 metros\",\n",
        "    \"0.75 mts de frente y 1.10 mts de fondo.\",\n",
        "    \"1.20 metros de ancho mínimo\",\n",
        "    \"Las rampas peatonales deben tener un ancho mínimo de 1.00 m y pendiente máxima de 10%\"\n",
        "]\n",
        "\n",
        "\n",
        "# Load necessary data\n",
        "def load_json(filepath):\n",
        "    \"\"\"Loads a JSON file with error handling\"\"\"\n",
        "    if os.path.exists(filepath):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath}: {e}\")\n",
        "    print(f\"File {filepath} not found\")\n",
        "    return {}\n",
        "\n",
        "# Calculate similarity between response and ground truth\n",
        "def calculate_similarity(response, ground_truth, similarity_model=None, rouge_evaluator=None):\n",
        "    \"\"\"Calculates similarity between response and ground truth\"\"\"\n",
        "    metrics = {'cosine_similarity': 0.0, 'rougeL_f': 0.0}\n",
        "\n",
        "    # Validate inputs\n",
        "    response = str(response) if response else \"\"\n",
        "    ground_truth = str(ground_truth) if ground_truth else \"\"\n",
        "\n",
        "    if not response or not ground_truth or not similarity_model:\n",
        "        return metrics\n",
        "\n",
        "    # Cosine similarity\n",
        "    try:\n",
        "        response_embedding = similarity_model.encode([response], convert_to_tensor=True)\n",
        "        ground_truth_embedding = similarity_model.encode([ground_truth], convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(response_embedding, ground_truth_embedding)[0]\n",
        "        metrics['cosine_similarity'] = float(cos_scores.item())\n",
        "    except Exception as e:\n",
        "        print(f\"Error in cosine similarity: {e}\")\n",
        "\n",
        "    # ROUGE-L\n",
        "    if rouge_evaluator:\n",
        "        try:\n",
        "            scores = rouge_evaluator.score(ground_truth, response)\n",
        "            metrics['rougeL_f'] = float(scores['rougeL'].fmeasure)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ROUGE: {e}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Corrected TF-IDF Search\n",
        "def search_by_tfidf(query, text_chunks):\n",
        "    \"\"\"Performs TF-IDF search on chunks\"\"\"\n",
        "    if not text_chunks or not query:\n",
        "        return \"No data for search\", 0.0\n",
        "\n",
        "    text_chunks_str = [str(chunk) for chunk in text_chunks if chunk]\n",
        "    query_str = str(query)\n",
        "\n",
        "    try:\n",
        "        # Ensure there is text to process\n",
        "        if not text_chunks_str:\n",
        "            return \"No valid chunks\", 0.0\n",
        "\n",
        "        # Create vectorizer without stop words (for higher precision)\n",
        "        vectorizer = TfidfVectorizer()\n",
        "\n",
        "        # Try to fit the vectorizer\n",
        "        try:\n",
        "            tfidf_chunks = vectorizer.fit_transform(text_chunks_str)\n",
        "        except ValueError:\n",
        "            # If it fails, try with a simpler vectorizer\n",
        "            vectorizer = TfidfVectorizer(lowercase=True, analyzer='word')\n",
        "            tfidf_chunks = vectorizer.fit_transform(text_chunks_str)\n",
        "\n",
        "        # Transform the query\n",
        "        tfidf_query = vectorizer.transform([query_str])\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = sklearn_cosine_similarity(tfidf_query, tfidf_chunks)[0]\n",
        "\n",
        "        # Verify that similarities were calculated\n",
        "        if similarities.size == 0:\n",
        "            return \"Similarities were not calculated\", 0.0\n",
        "\n",
        "        # Get the best result\n",
        "        best_index = np.argmax(similarities)\n",
        "        best_score = similarities[best_index]\n",
        "\n",
        "        # Return the full result\n",
        "        return text_chunks_str[best_index], float(best_score)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Detailed error in TF-IDF: {type(e).__name__}: {e}\")\n",
        "        return f\"TF-IDF Error: {type(e).__name__}\", 0.0\n",
        "\n",
        "# Function to determine the source (PDF/TXT) from the collection name\n",
        "def determine_source(collection_name, collection_key):\n",
        "    \"\"\"Determines if the source is PDF or TXT based on the collection name and key\"\"\"\n",
        "    collection_name = collection_name.lower()\n",
        "    collection_key = collection_key.lower()\n",
        "\n",
        "    # First, try using the collection name\n",
        "    if \"pdf\" in collection_name:\n",
        "        return \"PDF\"\n",
        "    elif \"text\" in collection_name or \"txt\" in collection_name:\n",
        "        return \"TXT\"\n",
        "\n",
        "    # If not found in the name, try with the key\n",
        "    if \"pdf\" in collection_key:\n",
        "        return \"PDF\"\n",
        "    elif \"text\" in collection_key or \"txt\" in collection_key:\n",
        "        return \"TXT\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Main function to evaluate models\n",
        "def evaluate_models_per_db():\n",
        "    \"\"\"Evaluates all models for each created database\"\"\"\n",
        "    print(\"Starting evaluation of models per database...\")\n",
        "\n",
        "    # Define files to use\n",
        "    COLLECTIONS_FILE = \"created_collections.json\"\n",
        "    TEXT_CHUNKS_FILE = \"chunks_texts.json\"\n",
        "\n",
        "    # Load necessary data\n",
        "    collections = load_json(COLLECTIONS_FILE)\n",
        "    text_chunks_data = load_json(TEXT_CHUNKS_FILE)\n",
        "\n",
        "    if not collections:\n",
        "        print(\"No collections to evaluate\")\n",
        "        return\n",
        "\n",
        "    # Initialize similarity model and ROUGE\n",
        "    similarity_model = None\n",
        "    try:\n",
        "        similarity_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        print(\"Similarity model initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing similarity model: {e}\")\n",
        "        return\n",
        "\n",
        "    rouge_evaluator = None\n",
        "    try:\n",
        "        from rouge_score import rouge_scorer\n",
        "        rouge_evaluator = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "        print(\"ROUGE evaluator initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"Rouge not available: {e}\")\n",
        "\n",
        "    # Prepare results\n",
        "    results = []\n",
        "\n",
        "    # Evaluate each collection\n",
        "    for key, collection_name in collections.items():\n",
        "        print(f\"\\nEvaluating collection: {key} -> {collection_name}\")\n",
        "\n",
        "        # Check if we have the texts for this collection\n",
        "        if collection_name not in text_chunks_data:\n",
        "            print(f\"No texts found for collection {collection_name}\")\n",
        "            continue\n",
        "\n",
        "        chunks = text_chunks_data[collection_name]\n",
        "        print(f\"  Found {len(chunks)} chunks to evaluate\")\n",
        "\n",
        "        # Determine embedding type\n",
        "        if \"_openai\" in key:\n",
        "            embedding_type = \"openai\"\n",
        "            print(\"  Using OpenAI embeddings\")\n",
        "        elif \"_google\" in key:\n",
        "            embedding_type = \"google\"\n",
        "            print(\"  Using Google embeddings\")\n",
        "        else:\n",
        "            print(f\"Cannot determine embedding type for {key}\")\n",
        "            continue\n",
        "\n",
        "        # Determine source (PDF or TXT) from name and key\n",
        "        source = determine_source(collection_name, key)\n",
        "        print(f\"  Source: {source}\")\n",
        "\n",
        "        # Evaluate questions\n",
        "        for i, query in enumerate(questions):\n",
        "            print(f\"\\n  Question {i+1}: {query}\")\n",
        "            print(f\"  Expected answer: {correct_answers[i]}\")\n",
        "\n",
        "            # TF-IDF method (keyword-based)\n",
        "            start_time = time.time()\n",
        "            response_tfidf, score_tfidf = search_by_tfidf(query, chunks)\n",
        "            time_tfidf = time.time() - start_time\n",
        "\n",
        "            print(f\"  • TF-IDF (score: {score_tfidf:.4f}, time: {time_tfidf:.2f}s)\")\n",
        "            print(f\"    TF-IDF Response:\")\n",
        "            print(f\"    '{response_tfidf}'\")\n",
        "\n",
        "            # Evaluate similarity with the correct answer\n",
        "            sim_tfidf = calculate_similarity(response_tfidf, correct_answers[i],\n",
        "                                             similarity_model, rouge_evaluator)\n",
        "\n",
        "            print(f\"TF-IDF Metrics: CosSim={sim_tfidf['cosine_similarity']:.4f}, RougeL={sim_tfidf['rougeL_f']:.4f}\")\n",
        "\n",
        "            # 2. Embeddings + vectorstore method\n",
        "            try:\n",
        "                # Create embeddings\n",
        "                if embedding_type == \"openai\":\n",
        "                    embeddings = OpenAIEmbeddings(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "                else:  # google\n",
        "                    embeddings = GoogleGenerativeAIEmbeddings(\n",
        "                        model=\"models/embedding-001\",\n",
        "                        api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
        "                    )\n",
        "\n",
        "                # Load vectorstore\n",
        "                vector_store = Chroma(\n",
        "                    collection_name=collection_name,\n",
        "                    embedding_function=embeddings\n",
        "                )\n",
        "\n",
        "                # Perform vector search\n",
        "                start_time = time.time()\n",
        "                vector_results = vector_store.similarity_search_with_score(query, k=1)\n",
        "                time_vector = time.time() - start_time\n",
        "\n",
        "                if vector_results:\n",
        "                    doc, score_vector = vector_results[0]\n",
        "                    response_vector = doc.page_content\n",
        "                    print(f\"  • Vector (score: {score_vector:.4f}, time: {time_vector:.2f}s)\")\n",
        "                    print(f\"    Vector Response:\")\n",
        "                    print(f\"    '{response_vector}'\")\n",
        "\n",
        "                    # Evaluate similarity with the correct answer\n",
        "                    sim_vector = calculate_similarity(response_vector, correct_answers[i],\n",
        "                                                      similarity_model, rouge_evaluator)\n",
        "\n",
        "                    print(f\"    Vector Metrics: CosSim={sim_vector['cosine_similarity']:.4f}, RougeL={sim_vector['rougeL_f']:.4f}\")\n",
        "                else:\n",
        "                    response_vector = \"No results found\"\n",
        "                    score_vector = 0.0\n",
        "                    sim_vector = {'cosine_similarity': 0.0, 'rougeL_f': 0.0}\n",
        "                    print(\"    No results found with vectorstore\")\n",
        "\n",
        "                # Save result\n",
        "                results.append({\n",
        "                    'Collection': collection_name,\n",
        "                    'Embedding_Type': embedding_type,\n",
        "                    'Source': source,\n",
        "                    'Question': query,\n",
        "                    'Correct_Answer': correct_answers[i],\n",
        "                    'Response_TFIDF': response_tfidf,\n",
        "                    'Score_TFIDF': score_tfidf,\n",
        "                    'Time_TFIDF': time_tfidf,\n",
        "                    'CosSim_TFIDF': sim_tfidf['cosine_similarity'],\n",
        "                    'RougeL_TFIDF': sim_tfidf['rougeL_f'],\n",
        "                    'Response_Vector': response_vector,\n",
        "                    'Score_Vector': float(score_vector),\n",
        "                    'Time_Vector': time_vector,\n",
        "                    'CosSim_Vector': sim_vector['cosine_similarity'],\n",
        "                    'RougeL_Vector': sim_vector['rougeL_f']\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating with vectorstore: {e}\")\n",
        "\n",
        "                # Save partial result\n",
        "                results.append({\n",
        "                    'Collection': collection_name,\n",
        "                    'Embedding_Type': embedding_type,\n",
        "                    'Source': source,\n",
        "                    'Question': query,\n",
        "                    'Correct_Answer': correct_answers[i],\n",
        "                    'Response_TFIDF': response_tfidf,\n",
        "                    'Score_TFIDF': score_tfidf,\n",
        "                    'Time_TFIDF': time_tfidf,\n",
        "                    'CosSim_TFIDF': sim_tfidf['cosine_similarity'],\n",
        "                    'RougeL_TFIDF': sim_tfidf['rougeL_f'],\n",
        "                    'Response_Vector': \"Error\",\n",
        "                    'Score_Vector': 0.0,\n",
        "                    'Time_Vector': 0.0,\n",
        "                    'CosSim_Vector': 0.0,\n",
        "                    'RougeL_Vector': 0.0\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame and save results\n",
        "    if results:\n",
        "        df = pd.DataFrame(results)\n",
        "        df_csv = df.copy()\n",
        "        df_csv.to_csv('model_evaluation.csv', index=False)\n",
        "\n",
        "        # Save the entire complete DataFrame as pickle to preserve full data\n",
        "        df.to_pickle('model_evaluation_complete.pkl')\n",
        "\n",
        "        print(\"\\nEvaluation completed.\")\n",
        "        print(\"  • Results saved in 'model_evaluation.csv'\")\n",
        "        print(\"  • Complete data saved in 'model_evaluation_complete.pkl'\")\n",
        "\n",
        "        # Show summary\n",
        "        print(\"\\nRESULTS SUMMARY:\")\n",
        "        print(f\"Total evaluations: {len(results)}\")\n",
        "\n",
        "        # Averages by embedding type\n",
        "        print(\"\\nAverage by embedding type:\")\n",
        "        for e_type in df['Embedding_Type'].unique():\n",
        "            subset = df[df['Embedding_Type'] == e_type]\n",
        "            print(f\"  • {e_type}:\")\n",
        "            print(f\"    - TF-IDF: CosSim={subset['CosSim_TFIDF'].mean():.4f}, RougeL={subset['RougeL_TFIDF'].mean():.4f}\")\n",
        "            print(f\"    - Vector: CosSim={subset['CosSim_Vector'].mean():.4f}, RougeL={subset['RougeL_Vector'].mean():.4f}\")\n",
        "\n",
        "        # Comparative table PDF vs TXT by embedding type\n",
        "        print(\"\\nGenerating comparative tables PDF vs TXT...\")\n",
        "\n",
        "        # Check if we have source data and multiple source types\n",
        "        if 'Source' in df.columns and len(df['Source'].unique()) > 1:\n",
        "            # Full table grouped by Source and Embedding_Type\n",
        "            comparison_table = df.groupby(['Source', 'Embedding_Type']).agg({\n",
        "                'CosSim_TFIDF': 'mean',\n",
        "                'RougeL_TFIDF': 'mean',\n",
        "                'CosSim_Vector': 'mean',\n",
        "                'RougeL_Vector': 'mean',\n",
        "                'Time_TFIDF': 'mean',\n",
        "                'Time_Vector': 'mean'\n",
        "            })\n",
        "\n",
        "            print(\"\\nCOMPARATIVE TABLE BY SOURCE AND EMBEDDING TYPE:\")\n",
        "            print(comparison_table)\n",
        "\n",
        "            # Save the table to a separate CSV file\n",
        "            comparison_table.to_csv('source_embedding_comparison.csv')\n",
        "            print(\"Comparative table saved in 'source_embedding_comparison.csv'\")\n",
        "\n",
        "            # Performance table by method (vector vs tf-idf)\n",
        "            table_by_method = pd.pivot_table(\n",
        "                df,\n",
        "                values=['CosSim_TFIDF', 'CosSim_Vector', 'RougeL_TFIDF', 'RougeL_Vector'],\n",
        "                index=['Source'],\n",
        "                columns=['Embedding_Type'],\n",
        "                aggfunc='mean'\n",
        "            )\n",
        "\n",
        "            print(\"\\nPERFORMANCE COMPARISON TABLE BY METHOD AND SOURCE:\")\n",
        "            print(table_by_method)\n",
        "            table_by_method.to_csv('full_performance_comparison.csv')\n",
        "            print(\"Complete comparative table saved in 'full_performance_comparison.csv'\")\n",
        "\n",
        "            metrics_list = []\n",
        "            for source_type in df['Source'].unique():\n",
        "                for e_type in df['Embedding_Type'].unique():\n",
        "                    subset = df[(df['Source'] == source_type) & (df['Embedding_Type'] == e_type)]\n",
        "\n",
        "                    # Calculate averages for each combination\n",
        "                    if not subset.empty: # Ensure subset exists\n",
        "                        metrics_list.append({\n",
        "                            'Source': source_type,\n",
        "                            'Embedding': e_type,\n",
        "                            'CosSim_TFIDF': subset['CosSim_TFIDF'].mean(),\n",
        "                            'CosSim_Vector': subset['CosSim_Vector'].mean(),\n",
        "                            'RougeL_TFIDF': subset['RougeL_TFIDF'].mean(),\n",
        "                            'RougeL_Vector': subset['RougeL_Vector'].mean()\n",
        "                        })\n",
        "\n",
        "            # Create DataFrame for the final table\n",
        "            if metrics_list:\n",
        "                final_table = pd.DataFrame(metrics_list)\n",
        "\n",
        "                print(\"\\nFINAL PERFORMANCE TABLE (easy-to-read format):\")\n",
        "                print(final_table)\n",
        "\n",
        "                # Save the final table to CSV\n",
        "                final_table.to_csv('final_performance_table.csv', index=False)\n",
        "                print(\"Final table saved in 'final_performance_table.csv'\")\n",
        "            else:\n",
        "                print(\"Could not generate final performance table (no valid data combinations found).\")\n",
        "\n",
        "        else:\n",
        "            print(\"Cannot generate comparative tables: missing 'Source' data or only one source type exists.\")\n",
        "    else:\n",
        "        print(\"No evaluation results were obtained\")\n",
        "\n",
        "# To run the evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_models_per_db()"
      ],
      "metadata": {
        "id": "uElGAvLGQY-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelos = {}\n",
        "\n",
        "# Inicializar OpenAI\n",
        "try:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    modelos[\"GPT-4\"] = ChatOpenAI(\n",
        "        model_name=\"gpt-4.1-mini\",\n",
        "        temperature=0.0,\n",
        "        max_tokens=2000,\n",
        "        api_key=OPENAI_API_KEY\n",
        "    )\n",
        "    print(\"Modelo GPT-4 inicializado correctamente\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al inicializar GPT-4: {str(e)}\")\n",
        "\n",
        "# Inicializar Claude\n",
        "try:\n",
        "    from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "    modelos[\"Claude\"] = ChatAnthropic(\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        temperature=0.0,\n",
        "        max_tokens_to_sample=2000,\n",
        "        api_key=ANTHROPIC_API_KEY\n",
        "    )\n",
        "    print(\"Modelo Claude inicializado correctamente\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al inicializar Claude: {str(e)}\")\n",
        "\n",
        "# Inicializar Gemini\n",
        "try:\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "    modelos[\"Gemini\"] = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        temperature=0.0,\n",
        "        max_output_tokens=2000,\n",
        "        google_api_key=GOOGLE_API_KEY\n",
        "    )\n",
        "    print(\"Modelo Gemini inicializado correctamente\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al inicializar Gemini: {str(e)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeN8006nUY-d",
        "outputId": "ff60fc53-e67c-49fa-f99d-0ca848e262cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo GPT-4 inicializado correctamente\n",
            "Modelo Claude inicializado correctamente\n",
            "Modelo Gemini inicializado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the similarity model\n",
        "similarity_model = None\n",
        "try:\n",
        "    similarity_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "    print(\"MPNet model loaded\")\n",
        "except Exception:\n",
        "    try:\n",
        "        similarity_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        print(\"MiniLM model loaded (alternative)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading similarity models: {e}\")\n",
        "\n",
        "# Initialize the ROUGE evaluator\n",
        "rouge_evaluator = None\n",
        "try:\n",
        "    print(\"ROUGE evaluator initialized (placeholder - requires 'rouge_scorer')\")\n",
        "except NameError:\n",
        "     print(\"Skipping ROUGE evaluator initialization: 'rouge_scorer' not defined.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error with ROUGE: {e}\")\n",
        "\n",
        "# Load the collections file\n",
        "COLLECTIONS_FILE = 'created_collections.json'\n",
        "\n",
        "# Load collections from the JSON file\n",
        "try:\n",
        "    with open(COLLECTIONS_FILE, 'r', encoding='utf-8') as f:\n",
        "        collections_data = json.load(f)\n",
        "    print(f\"Loaded {len(collections_data)} collections from '{COLLECTIONS_FILE}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading collections file: {e}\")\n",
        "    collections_data = {}\n",
        "\n",
        "# Main evaluation loop\n",
        "results = []\n",
        "\n",
        "# Select collections to evaluate\n",
        "selected_collections = []\n",
        "\n",
        "# If we don't select specific ones (list is empty), use all available\n",
        "if len(selected_collections) < 1:\n",
        "    selected_collections = [(k, v) for k, v in collections_data.items()]\n",
        "    print(\"Using all available collections\")\n",
        "\n",
        "# Instruction template (now using PromptTemplate)\n",
        "template = \"\"\"Answer the following question using ONLY the information provided in the context.\n",
        "If the complete answer is not found in the context, say \"I cannot answer based on the provided context.\" Do not add anything else.\n",
        "Do not invent information and keep your answer concise and direct.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Create PromptTemplate object\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Evaluate each combination\n",
        "for key, collection_name in selected_collections:\n",
        "    print(f\"\\nEvaluating collection: {key} -> {collection_name}\")\n",
        "\n",
        "    # Determine embedding type\n",
        "    try:\n",
        "        if \"_openai\" in key:\n",
        "            embeddings = OpenAIEmbeddings(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "            embedding_type = \"openai\"\n",
        "        elif \"_google\" in key:\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(\n",
        "                model=\"models/embedding-001\",\n",
        "                api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
        "            )\n",
        "            embedding_type = \"google\"\n",
        "        else:\n",
        "            # Fallback or default case\n",
        "            print(f\"Unknown embedding type pattern in key '{key}', using OpenAI as fallback\")\n",
        "            embeddings = OpenAIEmbeddings(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "            embedding_type = \"openai_fallback\"\n",
        "\n",
        "        # Load vector store\n",
        "        vector_store = Chroma(\n",
        "            collection_name=collection_name,\n",
        "            embedding_function=embeddings\n",
        "        )\n",
        "\n",
        "        # Create retriever\n",
        "        retriever = vector_store.as_retriever(\n",
        "            search_kwargs={\"k\": 5} # Retrieve top 5 documents\n",
        "        )\n",
        "\n",
        "\n",
        "        for llm_name, llm_instance in models.items():\n",
        "            print(f\"\\nEvaluating LLM: {llm_name} with retriever: {embedding_type}\")\n",
        "\n",
        "            try:\n",
        "                # Configure QA chain with the correct prompt\n",
        "                qa_chain = RetrievalQA.from_chain_type(\n",
        "                    llm=llm_instance,\n",
        "                    chain_type=\"stuff\",\n",
        "                    retriever=retriever,\n",
        "                    chain_type_kwargs={\n",
        "                        \"prompt\": prompt_template,\n",
        "                        \"verbose\": False\n",
        "                    },\n",
        "                    return_source_documents=True\n",
        "                )\n",
        "\n",
        "                for i, question in enumerate(questions):\n",
        "                    print(f\"  Question {i+1}: {question}\")\n",
        "\n",
        "                    try:\n",
        "                        # Measure time\n",
        "                        start_time = time.time()\n",
        "\n",
        "                        result_data = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "                        # Calculate time\n",
        "                        total_time = time.time() - start_time\n",
        "\n",
        "                        # Get answer and documents\n",
        "                        answer = result_data[\"result\"]\n",
        "                        documents = result_data.get(\"source_documents\", [])\n",
        "\n",
        "                        print(f\"  Time: {total_time:.2f}s\")\n",
        "                        print(f\"  Answer: {answer}\")\n",
        "\n",
        "                        sim_scores = calculate_similarity(answer, correct_answers[i], similarity_model)\n",
        "\n",
        "                        similarity_score = sim_scores.get('cosine_similarity', 0.0) # Default to 0 if key missing\n",
        "                        print(f\"  Similarity: {similarity_score:.4f}\")\n",
        "\n",
        "                        # Save result\n",
        "                        results.append({\n",
        "                            'LLM': llm_name,\n",
        "                            'Retriever': embedding_type,\n",
        "                            'Collection': collection_name,\n",
        "                            'Question': question,\n",
        "                            'LLM_Answer': answer,\n",
        "                            'Correct_Answer': correct_answers[i],\n",
        "                            'Time': total_time,\n",
        "                            'Similarity': similarity_score,\n",
        "                            'Documents_Used': len(documents)\n",
        "                        })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Error processing question '{question}': {str(e)}\")\n",
        "\n",
        "                        # Record error\n",
        "                        results.append({\n",
        "                            'LLM': llm_name,\n",
        "                            'Retriever': embedding_type,\n",
        "                            'Collection': collection_name,\n",
        "                            'Question': question,\n",
        "                            'LLM_Answer': f\"ERROR: {str(e)}\",\n",
        "                            'Correct_Answer': correct_answers[i],\n",
        "                            'Time': 0,\n",
        "                            'Similarity': 0.0,\n",
        "                            'Documents_Used': 0\n",
        "                        })\n",
        "            except Exception as e:\n",
        "                print(f\"Error configuring QA chain for {llm_name} with {collection_name}: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing collection {collection_name} (key: {key}): {str(e)}\")\n",
        "\n",
        "\n",
        "# Save and analyze results\n",
        "if results:\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Save results\n",
        "    df.to_csv('evaluacion_llms.csv', index=False)\n",
        "    df.to_pickle('evaluacion_llms.pkl')\n",
        "\n",
        "    print(\"\\nEvaluation completed. Results saved to 'evaluacion_llms.csv' and 'evaluacion_llms.pkl'.\")\n",
        "\n",
        "    # Display summary\n",
        "    print(\"\\nRESULTS SUMMARY:\")\n",
        "\n",
        "    # By LLM\n",
        "    print(\"\\nPerformance by LLM:\")\n",
        "    if 'LLM' in df.columns and 'Similarity' in df.columns and 'Time' in df.columns:\n",
        "        for llm_name_summary in df['LLM'].unique():\n",
        "            subset = df[df['LLM'] == llm_name_summary]\n",
        "            print(f\"  • {llm_name_summary}:\")\n",
        "            print(f\"    - Average Similarity: {subset['Similarity'].mean():.4f}\")\n",
        "            print(f\"    - Average Time: {subset['Time'].mean():.2f}s\")\n",
        "    else:\n",
        "        print(\"  Could not generate LLM summary - required columns missing.\")\n",
        "\n",
        "\n",
        "    # By Retriever Type\n",
        "    print(\"\\nPerformance by Retriever Type:\")\n",
        "    if 'Retriever' in df.columns and 'Similarity' in df.columns:\n",
        "        for retriever_type in df['Retriever'].unique():\n",
        "            subset = df[df['Retriever'] == retriever_type]\n",
        "            print(f\"  • {retriever_type}:\")\n",
        "            print(f\"    - Average Similarity: {subset['Similarity'].mean():.4f}\")\n",
        "    else:\n",
        "         print(\"  Could not generate Retriever summary - required columns missing.\")\n",
        "\n",
        "\n",
        "    # Best Combinations\n",
        "    print(\"\\nTop 3 Best LLM+Retriever Combinations (by average similarity):\")\n",
        "    if 'LLM' in df.columns and 'Retriever' in df.columns and 'Similarity' in df.columns and 'Time' in df.columns:\n",
        "        best_combinations = df.groupby(['LLM', 'Retriever']).agg(\n",
        "            Avg_Similarity=('Similarity', 'mean'),\n",
        "            Avg_Time=('Time', 'mean')\n",
        "        ).reset_index().sort_values('Avg_Similarity', ascending=False).head(3)\n",
        "\n",
        "        for i, row in best_combinations.iterrows():\n",
        "            print(f\"  {i+1}. {row['LLM']} + {row['Retriever']}:\")\n",
        "            print(f\"     - Avg Similarity: {row['Avg_Similarity']:.4f}\")\n",
        "            print(f\"     - Avg Time: {row['Avg_Time']:.2f}s\")\n",
        "    else:\n",
        "        print(\"  Could not generate Best Combinations summary - required columns missing.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo results were obtained during the evaluation.\")"
      ],
      "metadata": {
        "id": "AMlbTcXITWgN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}